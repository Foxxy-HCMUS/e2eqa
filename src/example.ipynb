{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b845a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'graph_tool', 'wurlitzer', 'infomap', 'bayanpy', 'leidenalg'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'ASLPAw', 'pyclustering'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'leidenalg', 'infomap', 'wurlitzer'}\n"
     ]
    }
   ],
   "source": [
    "from models.pairwise_model import *\n",
    "from features.text_utils import *\n",
    "import regex as re\n",
    "from models.bm25_utils import BM25Gensim\n",
    "from models.qa_model import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "609ffe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a8caca2b834b78a6e42d5deb35e3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_titles = []\n",
    "all_texts = []\n",
    "with open(\"../data/raw/wikipedia_20220620_cleaned/wikipedia_20220620_cleaned.jsonl\", encoding=\"utf-8\") as f:\n",
    "    for i,line in tqdm(enumerate(f)):\n",
    "        x = json.loads(line)\n",
    "        if x['title'] not in['None', 'NaN']:\n",
    "            all_titles.append(x[\"title\"])\n",
    "        else: \n",
    "            all_titles.append('nan')\n",
    "        all_texts.append(x[\"text\"][:200])\n",
    "        \n",
    "df_wiki_all = pd.DataFrame()\n",
    "df_wiki_all[\"title\"] = all_titles\n",
    "df_wiki_all[\"text\"] = all_texts\n",
    "\n",
    "# df_wiki_all.to_csv(\"../data/wikipedia_20220620_short.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f46791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki_windows = pd.read_csv(\"../data/processed/wikipedia_20220620_cleaned_v2.csv\")\n",
    "df_wiki = pd.read_csv(\"../data/wikipedia_20220620_short.csv\")\n",
    "df_wiki.title = df_wiki.title.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "622368d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1273469, 2), (1944406, 3), (1273469, 2))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.shape, df_wiki_windows.shape, df_wiki_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94147c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = json.load(open(\"../data/processed/entities.json\"))\n",
    "new_dict = dict()\n",
    "for key, val in entity_dict.items():\n",
    "    val = val.replace(\"wiki/\", \"\").replace(\"_\", \" \")\n",
    "    entity_dict[key] = val\n",
    "    key = preprocess(key)\n",
    "    new_dict[key.lower()] = val\n",
    "entity_dict.update(new_dict)\n",
    "title2idx = dict([(x.strip(), y) for x, y in zip(df_wiki.title, df_wiki.index.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9289cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/vi-mrc-base were not used when initializing RobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# qa_model = QAEnsembleModel(\"nguyenvulebinh/vi-mrc-large\", [\"../models/qa_model_robust.bin\"], entity_dict)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pairwise_model_stage1 \u001b[39m=\u001b[39m PairwiseModel(\u001b[39m\"\u001b[39;49m\u001b[39mnguyenvulebinh/vi-mrc-base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mhalf()\n\u001b[0;32m      3\u001b[0m pairwise_model_stage1\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m../models/pairwise_v2.bin\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m pairwise_model_stage1\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\Downloads\\Nam 3\\Second Semester\\statistical learning\\final project\\e2eqa\\src\\models\\pairwise_model.py:23\u001b[0m, in \u001b[0;36mPairwiseModel.__init__\u001b[1;34m(self, model_name, max_length, batch_size, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(model_name, use_auth_token\u001b[39m=\u001b[39mAUTH_TOKEN)\n\u001b[1;32m---> 23\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name, use_auth_token\u001b[39m=\u001b[39mAUTH_TOKEN)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\transformers\\modeling_utils.py:1902\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1898\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1899\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1900\u001b[0m     )\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Envs\\e2eqa\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# qa_model = QAEnsembleModel(\"nguyenvulebinh/vi-mrc-large\", [\"../models/qa_model_robust.bin\"], entity_dict)\n",
    "pairwise_model_stage1 = PairwiseModel(\"nguyenvulebinh/vi-mrc-base\").half()\n",
    "pairwise_model_stage1.load_state_dict(torch.load(\"../models/pairwise_v2.bin\"))\n",
    "pairwise_model_stage1.eval()\n",
    "\n",
    "pairwise_model_stage2 = PairwiseModel(\"nguyenvulebinh/vi-mrc-base\").half()\n",
    "pairwise_model_stage2.load_state_dict(torch.load(\"../models/pairwise_stage2_seed0.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0754ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_model_stage1 = BM25Gensim(\"../models/bm25_stage1/\", entity_dict, title2idx)\n",
    "bm25_model_stage2_full = BM25Gensim(\"../models/bm25_stage2/full_text/\", entity_dict, title2idx)\n",
    "bm25_model_stage2_title = BM25Gensim(\"../models/bm25_stage2/title/\", entity_dict, title2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6431b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Nội nhũ là gì?\"\n",
    "#Bm25 retrieval for top200 candidates\n",
    "query = preprocess(question).lower()\n",
    "top_n, bm25_scores = bm25_model_stage1.get_topk_stage1(query, topk=200)\n",
    "titles = [preprocess(df_wiki_windows.title.values[i]) for i in top_n]\n",
    "texts = [preprocess(df_wiki_windows.text.values[i]) for i in top_n]\n",
    "\n",
    "#Reranking with pairwise model for top10\n",
    "question = preprocess(question)\n",
    "ranking_preds = pairwise_model_stage1.stage1_ranking(question, texts)\n",
    "ranking_scores = ranking_preds * bm25_scores\n",
    "\n",
    "#Question answering\n",
    "best_idxs = np.argsort(ranking_scores)[-10:]\n",
    "ranking_scores = np.array(ranking_scores)[best_idxs]\n",
    "texts = np.array(texts)[best_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "938e33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_e2e(question):\n",
    "    #Bm25 retrieval for top200 candidates\n",
    "    query = preprocess(question).lower()\n",
    "    top_n, bm25_scores = bm25_model_stage1.get_topk_stage1(query, topk=200)\n",
    "    titles = [preprocess(df_wiki_windows.title.values[i]) for i in top_n]\n",
    "    texts = [preprocess(df_wiki_windows.text.values[i]) for i in top_n]\n",
    "    \n",
    "    #Reranking with pairwise model for top10\n",
    "    question = preprocess(question)\n",
    "    ranking_preds = pairwise_model_stage1.stage1_ranking(question, texts)\n",
    "    ranking_scores = ranking_preds * bm25_scores\n",
    "    \n",
    "    #Question answering\n",
    "    best_idxs = np.argsort(ranking_scores)[-10:]\n",
    "    ranking_scores = np.array(ranking_scores)[best_idxs]\n",
    "    texts = np.array(texts)[best_idxs]\n",
    "    best_answer = qa_model(question, texts, ranking_scores)\n",
    "    if best_answer is None:\n",
    "        return \"Chịu\"\n",
    "    bm25_answer = preprocess(str(best_answer).lower(), max_length=128, remove_puncts=True)\n",
    "    \n",
    "    #Entity mapping\n",
    "    if not check_number(bm25_answer):\n",
    "        bm25_question = preprocess(str(question).lower(), max_length=128, remove_puncts=True)\n",
    "        bm25_question_answer = bm25_question + \" \" + bm25_answer\n",
    "        candidates, scores = bm25_model_stage2_title.get_topk_stage2(bm25_answer, raw_answer=best_answer)\n",
    "        titles = [df_wiki.title.values[i] for i in candidates]\n",
    "        texts = [df_wiki.text.values[i] for i in candidates]\n",
    "        ranking_preds = pairwise_model_stage2.stage2_ranking(question, best_answer, titles, texts)\n",
    "        if ranking_preds.max() >= 0.1:\n",
    "            final_answer = titles[ranking_preds.argmax()]\n",
    "        else:\n",
    "            candidates, scores = bm25_model_stage2_full.get_topk_stage2(bm25_question_answer)\n",
    "            titles = [df_wiki.title.values[i] for i in candidates] + titles\n",
    "            texts = [df_wiki.text.values[i] for i in candidates] + texts\n",
    "            ranking_preds = np.concatenate(\n",
    "                [pairwise_model_stage2.stage2_ranking(question, best_answer, titles, texts), ranking_preds])\n",
    "        final_answer = \"wiki/\"+titles[ranking_preds.argmax()].replace(\" \",\"_\")\n",
    "    else:\n",
    "        final_answer = bm25_answer.lower()\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4433257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiki/J._R._R._Tolkien'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_e2e(\"Ai là tác giả Lord of the Rings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ff01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
